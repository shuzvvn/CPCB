4. values of betak,0
# What are the values of β k,0 for k ∈ {1, . . . , K} (bias terms of the second layer)
at the end of third epoch? Report numbers till 4 places of decimal and in the correct
order.
[-1.12184348 -0.61854864 -0.74667789  0.62800778 -0.29274996  0.14383504
 -0.64230133  0.13875552  2.07145084  0.4231668 ]


5.6. 
# List the average test loss at the end of each epoch. Report numbers till 4 places
of decimal.
# List the test accuracy at the end of each epoch. Report numbers till 4 places
of decimal.
epoch	train_l	test_l	test_ac
0	2.4036	1.5640	0.6630
1	1.1820	1.3888	0.6980
2	0.8984	1.2866	0.7090
3	0.7327	1.2232	0.7220
4	0.6194	1.1940	0.7240
5	0.5331	1.1684	0.7320
6	0.4654	1.1316	0.7350
7	0.4116	1.0895	0.7440
8	0.3680	1.0559	0.7500
9	0.3312	1.0290	0.7590
10	0.2999	1.0072	0.7630
11	0.2731	0.9902	0.7640
12	0.2498	0.9784	0.7630
13	0.2297	0.9708	0.7700
14	0.2122	0.9661	0.7690



7.
Run the model for 100 epochs, report the average final training loss and the
test accuracy. Report numbers till 4 places of decimal.
epoch	train_l	test_l	test_ac
0	2.4036	1.5640	0.6630
1	1.1820	1.3888	0.6980
2	0.8984	1.2866	0.7090
3	0.7327	1.2232	0.7220
4	0.6194	1.1940	0.7240
5	0.5331	1.1684	0.7320
6	0.4654	1.1316	0.7350
7	0.4116	1.0895	0.7440
8	0.3680	1.0559	0.7500
9	0.3312	1.0290	0.7590
10	0.2999	1.0072	0.7630
11	0.2731	0.9902	0.7640
12	0.2498	0.9784	0.7630
13	0.2297	0.9708	0.7700
14	0.2122	0.9661	0.7690
15	0.1970	0.9633	0.7720
16	0.1837	0.9616	0.7700
17	0.1719	0.9606	0.7670
18	0.1613	0.9601	0.7700
19	0.1518	0.9603	0.7710
20	0.1430	0.9611	0.7710
21	0.1349	0.9626	0.7690
22	0.1274	0.9644	0.7660
23	0.1204	0.9666	0.7690
24	0.1139	0.9687	0.7700
25	0.1078	0.9709	0.7720
26	0.1022	0.9732	0.7740
27	0.0970	0.9755	0.7760
28	0.0922	0.9780	0.7760
29	0.0877	0.9806	0.7750
30	0.0835	0.9834	0.7740
31	0.0796	0.9862	0.7740
32	0.0760	0.9891	0.7740
33	0.0726	0.9922	0.7750
34	0.0694	0.9953	0.7740
35	0.0664	0.9985	0.7730
36	0.0636	1.0017	0.7740
37	0.0610	1.0049	0.7740
38	0.0585	1.0081	0.7740
39	0.0562	1.0113	0.7730
40	0.0540	1.0146	0.7720
41	0.0519	1.0179	0.7720
42	0.0499	1.0211	0.7720
43	0.0481	1.0244	0.7710
44	0.0463	1.0277	0.7720
45	0.0447	1.0309	0.7710
46	0.0431	1.0341	0.7720
47	0.0416	1.0372	0.7720
48	0.0402	1.0403	0.7720
49	0.0389	1.0432	0.7730
50	0.0376	1.0460	0.7740
51	0.0364	1.0487	0.7750
52	0.0352	1.0513	0.7740
53	0.0341	1.0537	0.7740
54	0.0330	1.0559	0.7710
55	0.0320	1.0581	0.7710
56	0.0311	1.0600	0.7710
57	0.0302	1.0618	0.7710
58	0.0293	1.0634	0.7700
59	0.0285	1.0649	0.7710
60	0.0277	1.0663	0.7720
61	0.0270	1.0675	0.7720
62	0.0262	1.0686	0.7720
63	0.0256	1.0695	0.7730
64	0.0249	1.0704	0.7720
65	0.0243	1.0712	0.7720
66	0.0237	1.0718	0.7720
67	0.0231	1.0724	0.7720
68	0.0226	1.0729	0.7720
69	0.0221	1.0734	0.7730
70	0.0216	1.0737	0.7720
71	0.0211	1.0741	0.7720
72	0.0206	1.0743	0.7730
73	0.0202	1.0746	0.7730
74	0.0197	1.0748	0.7730
75	0.0193	1.0749	0.7740
76	0.0189	1.0751	0.7740
77	0.0186	1.0752	0.7750
78	0.0182	1.0753	0.7760
79	0.0178	1.0754	0.7750
80	0.0175	1.0754	0.7750
81	0.0172	1.0755	0.7740
82	0.0168	1.0755	0.7740
83	0.0165	1.0756	0.7740
84	0.0162	1.0756	0.7750
85	0.0160	1.0756	0.7740
86	0.0157	1.0756	0.7760
87	0.0154	1.0757	0.7760
88	0.0151	1.0757	0.7770
89	0.0149	1.0757	0.7770
90	0.0146	1.0757	0.7780
91	0.0144	1.0757	0.7790
92	0.0142	1.0758	0.7790
93	0.0139	1.0758	0.7790
94	0.0137	1.0758	0.7790
95	0.0135	1.0759	0.7790
96	0.0133	1.0759	0.7780
97	0.0131	1.0759	0.7800
98	0.0129	1.0760	0.7810
99	0.0127	1.0760	0.7810



9.
4 6 4
15 5 7
38 0 3
68 9 7



10